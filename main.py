#!/usr/bin/env python3
"""
AI Agent for Candidate Ranking using smolagents

This program creates an AI agent that can rank candidates for job descriptions
based on various criteria including skills, experience, education, and role relevance.

Usage:
    python main.py

Then interact with the agent through the terminal interface.
"""
import warnings
warnings.filterwarnings("ignore")
from pydantic.warnings import PydanticDeprecatedSince20

# Suppress Pydantic v2-specific deprecation warnings
warnings.filterwarnings("ignore", category=PydanticDeprecatedSince20)

# Suppress other deprecated API usage (e.g., httpx)
import os
import sys
from typing import Optional
from dotenv import load_dotenv
from smolagents import CodeAgent, InferenceClientModel, LiteLLMModel, TransformersModel
from fetch_emails_tool import fetch_recent_emails
from candidate_ranking_tool import (
    rank_candidates_for_job,
    list_available_jobs,
    get_job_details,
    search_candidates_by_skill
)
from pdf_extraction_tools import (
    extract_pdf_text,
    extract_pdf_urls,
    extract_pdf_contact_info,
    summarize_pdf,
    summarize_all_pdfs_in_directory,
    search_pdfs_for_skill
)

# Load environment variables
load_dotenv()


class CandidateRankingAgent:
    """
    Main agent class that handles candidate ranking operations.
    """

    def __init__(self):
        self.agent = None
        self.model = None
        self.setup_model()
        self.setup_agent()

    def setup_model(self):
        """
        Set up the language model for the agent.
        Priority: HuggingFace API > LiteLLM > Local Transformers
        """
        try:
            # Try HuggingFace API first (recommended)
            hf_token = os.getenv("HF_TOKEN")
            if hf_token:
                print("üîß Setting up HuggingFace API model...")
                self.model = InferenceClientModel(
                    model_id="Qwen/Qwen2.5-Coder-32B-Instruct",
                    token=hf_token
                )
                # check if it is working
                test_response = self.model.invoke("Hello", max_tokens=5)
                if not test_response or "error" in str(test_response).lower():
                    raise Exception("Hugging Face API is unavailable or quota exceeded.")
                
                print("‚úÖ HuggingFace API model ready!")
                return
        except Exception as e:
            print(f"‚ùå HuggingFace API setup failed: {e}")

        try:
            # Try LiteLLM with local models (ollama)
            print("üîß Setting up LiteLLM model (trying local ollama)...")
            self.model = LiteLLMModel(
                model_id="ollama/codellama:13b-instruct",
                system_message="""
You are a coding assistant that always replies using structured steps.

Follow this pattern:

1. Thought: Explain what you're going to do.
2. <code> block: Write the actual Python code using properly defined variables.
3. Output: Your final output is wrapped in `final_answer(...)` and must use only variables that were defined in the code block.

Example:

Thought: I will define a variable with a greeting message and return it.

<code>
greeting = "Hello, world!"
final_answer(greeting)
</code>

A nice workflow would be:
Thought: I will fetch emails using fetch_emails_tool and then read the files in emails/ folder using pdf_extraction_tools. then i will use candidate_ranking_tool to rank the candidates.
<code>
your decision
</code>

Rules:
- Never use a variable that was not defined.
- Never return code outside <code>...</code> tags.
- Always wrap final output inside final_answer(...).
- Always define all variables before use.
""",
allow_all_imports=True,
additional_authorized_imports=["os", "pdf_tools", "candidate_ranking_tool","glob"]
            )
            print("‚úÖ LiteLLM model ready!")
            return
        except Exception as e:
            print(f"‚ùå LiteLLM setup failed: {e}")

        try:
            # Fallback to local Transformers model
            print("üîß Setting up local Transformers model...")
            self.model = TransformersModel(
                model_id="microsoft/DialoGPT-medium"
            )
            print("‚úÖ Local Transformers model ready!")
            return
        except Exception as e:
            print(f"‚ùå Transformers setup failed: {e}")

        print("‚ùå Could not set up any model. Please check your configuration.")
        sys.exit(1)

    def setup_agent(self):
        """
        Set up the agent with our custom tools.
        """
        print("üîß Setting up agent with candidate ranking tools...")

        # Custom tools for candidate ranking and PDF processing
        tools = [
            rank_candidates_for_job,
            list_available_jobs,
            get_job_details,
            search_candidates_by_skill,
            extract_pdf_text,
            extract_pdf_urls,
            extract_pdf_contact_info,
            summarize_pdf,
            summarize_all_pdfs_in_directory,
            search_pdfs_for_skill,
            fetch_recent_emails
        ]

        self.agent = CodeAgent(
            tools=tools,
            model=self.model,
            add_base_tools=True,  # Add default tools like web search
            stream_outputs=True   # Enable streaming for better user experience
            
        )

        print("‚úÖ Agent ready with candidate ranking and PDF extraction capabilities!")

    def run_interactive_session(self):
        """
        Run an interactive session with the agent.
        """
        print("\n" + "="*60)
        print("ü§ñ AI CANDIDATE RANKING AGENT")
        print("="*60)
        print("\nWelcome! I can help you rank candidates and extract information from PDF resumes.")
        print("\nWhat I can do:")
        print("‚Ä¢ Rank candidates for specific job descriptions")
        print("‚Ä¢ Extract text, URLs, and contact info from PDF resumes")
        print("‚Ä¢ Summarize individual PDFs or entire directories")
        print("‚Ä¢ Search PDFs for specific skills or keywords")
        print("‚Ä¢ List available sample jobs and get job details")
        print("‚Ä¢ Search candidates by skills")

        print("\nSample commands you can try:")
        print("‚Ä¢ 'Give me a summary of all PDFs in the resumes directory'")
        print("‚Ä¢ 'Extract contact information from resumes/10228751.pdf'")
        print("‚Ä¢ 'Find all PDFs containing Python in the resumes folder'")
        print("‚Ä¢ 'Summarize the resume resumes/11676151.pdf'")
        print("‚Ä¢ 'Give me the 3 best fit candidates for job 1'")
        print("‚Ä¢ 'Show me all available jobs'")

        print("\nType 'quit' or 'exit' to end the session.")
        print("-" * 60)

        while True:
            try:
                user_input = input("\nüîç Your request: ").strip()

                if user_input.lower() in ['quit', 'exit', 'q']:
                    print("\nüëã Thank you for using the AI Candidate Ranking Agent!")
                    break

                if not user_input:
                    print("Please enter a request or 'quit' to exit.")
                    continue

                print(f"\nü§ñ Processing: {user_input}")
                print("-" * 40)

                # Run the agent
                response = self.agent.run(user_input)

                print(f"\nüìã Response:\n{response}")
                print("-" * 60)

            except KeyboardInterrupt:
                print("\n\nüëã Session interrupted. Goodbye!")
                break
            except Exception as e:
                print(f"\n‚ùå Error: {e}")
                print("Please try again or type 'quit' to exit.")

    def run_single_query(self, query: str) -> str:
        """
        Run a single query and return the response.

        Args:
            query: The user query

        Returns:
            The agent's response
        """
        try:
            return self.agent.run(query)
        except Exception as e:
            return f"Error processing query: {e}"


def print_setup_instructions():
    """
    Print setup instructions for the user.
    """
    print("\n" + "="*60)
    print("üöÄ SETUP INSTRUCTIONS")
    print("="*60)
    print("\nFor the best experience, set up your environment:")
    print("\n1. HuggingFace API (Recommended):")
    print("   - Get a free token from https://huggingface.co/settings/tokens")
    print("   - Set HF_TOKEN environment variable or create .env file:")
    print("     HF_TOKEN=your_token_here")

    print("\n2. Alternative: Local Ollama")
    print("   - Install Ollama: https://ollama.com/")
    print("   - Run: ollama pull llama3.2")
    print("   - Start ollama server: ollama serve")

    print("\n3. Install dependencies:")
    print("   pip install -r requirements.txt")
    print("\n" + "="*60)


def main():
    """
    Main entry point for the application.
    """
    print("üöÄ Starting AI Candidate Ranking Agent...")

    # Check if we have any model configuration
    if not os.getenv("HF_TOKEN"):
        print("\n‚ö†Ô∏è  No HuggingFace token found. The agent will try alternative models.")
        print("For best results, please set up a HuggingFace API token.")
        print_setup_instructions()

        # Ask user if they want to continue
        continue_anyway = input(
            "\nContinue without HuggingFace token? (y/n): ").lower()
        if continue_anyway != 'y':
            print("Please set up your environment and try again.")
            sys.exit(0)

    try:
        # Initialize the agent
        agent = CandidateRankingAgent()

        # Check if running with command line arguments
        if len(sys.argv) > 1:
            # Run single query from command line
            query = " ".join(sys.argv[1:])
            response = agent.run_single_query(query)
            print(f"\nüìã Response:\n{response}")
        else:
            # Run interactive session
            agent.run_interactive_session()

    except Exception as e:
        print(f"\n‚ùå Failed to start agent: {e}")
        print("\nPlease check your setup and try again.")
        sys.exit(1)


if __name__ == "__main__":
    main()
